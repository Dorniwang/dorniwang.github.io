<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Duomin Wang">
  <meta name="description" content="Duomin Wang's Homepage">
  <meta name="keywords" content="Duomin Wang,王多民,homepage,主页,computer vision,xiaobing.ai,Jilin University,talking head synthesis,representation learning,facial landmark detection,3D face reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="n8x1FW7r1Wku3m83FO96GU0x1exw46HxbsNW1IUM12U" />
  <title>Duomin Wang (王多民)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Duomin Wang (王多民)</name>
              </p>
              <p style="text-align:center">
                Email: wangduomin[at]gmail.com &nbsp; &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=a6GrP_EAAAAJ&hl=en">Google Scholar</a> &nbsp; &nbsp;&nbsp;&nbsp;<a href="https://github.com/Dorniwang">Github</a>
              </p>
              <p>I am currently a senior researcher at Stepfun from 2024. My research interests include video generation, avatar synthesis and driving, human-centric world model and representation learning. 
              </p>
              <p>
                Before joining Stepfun, I was a researcher in Xiaobing for nearly three years, work closely with Yu Deng and Baoyuan Wang. Before that, I was worked at OPPO Research Institute for three years, my research results are applied to the camera software of OPPO mobile phones as the basic face algorithm.
              </p>
              <p>
                <strong>
                <p>I'm seeking research interns on human-centric video & world model. Feel free to send me an email if you are interested. </p>
                <p>Let's explore together how to leverage world models to create more possibilities for digital human interactions.</p>
                </strong>
              </p>
		    
            </td>
            <td style="padding:15% 7% 7% 7%;width:40%;max-width:40%">
              <a href="images/duomin.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/duomin.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <!-- News -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <strong>2025/09/09</strong> &nbsp We have open-sourced the UniVerse-1. Check it out here <a href=https://huggingface.co/dorni/UniVerse-1-Base><img src="./images/huggingface.svg" height="20px"></a> <a href=https://github.com/Dorniwang/UniVerse-1-code><img src="./images/github.png" height="20px"></a>.
            </p>
            <p>
              <strong>2025/07/24</strong> &nbsp We have open-sourced the SpeakerVid-5M dataset and its data curation pipeline. Check it out here <a href=https://huggingface.co/datasets/dorni/SpeakerVid-5M-Dataset><img src="./images/huggingface.svg" height="20px"></a> <a href=https://github.com/Dorniwang/SpeakerVid-5M-Code><img src="./images/github.png" height="20px"></a>.
            </p>
            <p>
              <strong>2025/02/27</strong> &nbsp Had One paper accepted by CVPR 2025 about video prediction (<a href="https://magivideogen.github.io/">MAGI</a>).
            </p>
            <p>
              <strong>2024/08/08</strong> &nbsp Had One paper accepted by ECCV 2024 workshop EEC about agent avatar (<a href="https://dorniwang.github.io/AgentAvatar_project/">AgentAvatar</a>).
            </p>
            <p>
              <strong>2024/07/01</strong> &nbsp Had One paper accepted by ECCV 2024 about 4D avatar synthesis (<a href="https://yudeng.github.io/Portrait4D-v2/">Portrait4D-v2</a>).
            </p>
            <p>
              <strong>2024/02/27</strong> &nbsp Had two papers accepted by CVPR 2024, one is about 4D avatar synthesis (<a href="https://yudeng.github.io/Portrait4D/">Portrait4D</a>), the other one is about unconstrained virtural try-on (<a href="https://github.com/ningshuliang/PICTURE">PICTURE</a>).
            </p>
            <p>
              <strong>2023/07/14</strong> &nbsp Had one paper accepted by ICCV 2023 about talking head sythesis (<a href="https://zxyin.github.io/TH-PAD/">TH-PAD</a>).
            </p>
            <p>
              <strong>2023/07/10</strong> &nbsp Our CVPR 2023 work <a href="https://dorniwang.github.io/PD-FGC/">PD-FGC</a> has released the code and model, check it out!
            </p>
            <p>
              <strong>2023/02/28</strong> &nbsp Had one paper accepted by CVPR 2023 about talking head sythesis (<a href="https://dorniwang.github.io/PD-FGC/">PD-FGC</a>).
            </p>
          </td>
        </tr>
      </tbody></table>

      <!-- publications -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <!-- UniVerse-1 -->
        <tr></tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
              <source src='images/universe_1.mp4'>
            </video>
          </div>
      </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>UniVerse-1: Unified Audio-Video Generation via Stitching of Experts</papertitle>
              <br>
              <strong>Duomin Wang</strong>, Wei Zuo, Aojie Li, Ling-Hao Chen, Xinyao Liao, Deyu Zhou, Zixin Yin, Xili Dai, Daxin Jiang, Gang Yu 
              <br>
                <em>arXiv</em>, 2508.09131,
              <br>
              <a href="https://arxiv.org/abs/2509.06155">[PDF]</a>
              <a href="https://dorniwang.github.io/UniVerse-1">[Project]</a>
              <a href="https://github.com/Dorniwang/UniVerse-1-code">[Code]</a>
              <a href="https://huggingface.co/dorni/UniVerse-1-Base">[Model weights]</a>
              <a href="https://huggingface.co/datasets/dorni/Verse-Bench">[Verch-Ben]</a>
              <br>
              <p>We introduce UniVerse-1, a unified, Veo-3-like model capable of simultaneously generating coordinated audio and video. To enhance training efficiency, we bypass training from scratch and instead employ a stitching of expertise technique.</p>
          </td>
        </tr>
        
        <!-- ColorCtrl -->
        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='images/colorctrl.png' style="width:100%;max-width:100%; position: absolute;">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer</papertitle>
              <br>
              Zixin Yin, Xili Dai, Ling-Hao Chen, Deyu Zhou, Jianan Wang, <strong>Duomin Wang</strong>, Gang Yu, Lionel M. Ni, Lei Zhang, Heung-Yeung Shum
              <br>
                <em>arXiv</em>, 2508.09131,
              <br>
              <a href="https://arxiv.org/abs/2508.09131">[PDF]</a>
              <a href="https://zxyin.github.io/ColorCtrl">[Project]</a>
              <a>[Code(coming soon)]</a>
              <br>
              <p>We introduce ColorCtrl, a training-free method for text-guided color editing in images and videos. It enables precise, word-level control of color attributes while preserving geometry and material consistency. Experiments on SD3, FLUX.1-dev, and CogVideoX show that ColorCtrl outperforms existing training-free and commercial models, including GPT-4o and FLUX.1 Kontext Max, and generalizes well to instruction-based editing frameworks.</p>
          </td>
        </tr>
          
        <!-- SpeakerVid-5M -->
        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                  <source src='images/speakervid.mp4'>
                </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>SpeakerVid-5M: A Large-Scale High-Quality Dataset for audio-visual Dyadic Interactive Human Generation</papertitle>
            <br>
            Youliang Zhang, Zhaoyang Li, <strong>Duomin Wang</strong>, Jiahe Zhang, Deyu Zhou, Zixin Yin, Xili Dai, Gang Yu, Xiu Li
            <br>
            <em>arxiv 2507.09862</em>,
            <br>
            <a href="https://arxiv.org/abs/2507.09862">[PDF]</a>
            <a href="https://dorniwang.github.io/SpeakerVid-5M/">[Project]</a>
            <a href="https://github.com/Dorniwang/SpeakerVid-5M-Code">[Code]</a>
            <a href="https://huggingface.co/datasets/dorni/SpeakerVid-5M-Dataset">[Dataset]</a>
            <br>
            <p>We introduce SpeakerVid-5M, the first large-scale dataset designed specifically for the audio-visual dyadic interactive virtual human task</p>
          </td>
        </tr>

        <!-- MAGI -->
        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                  <source src='images/magi.mp4'>
                </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Taming Teacher Forcing for Masked Autoregressive Video Generation</papertitle>
            <br>
            Deyu Zhou, Quan Sun, Yuang Peng, Kun Yan, Runpei Dong, <strong>Duomin Wang</strong>, Zheng Ge, Nan Duan, Xiangyu Zhang, Lionel M. Ni, Heung-Yeung Shum
            <br>
            <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2025,
            <br>
            <a href="https://arxiv.org/abs/2501.12389">[PDF]</a>
            <a href="https://magivideogen.github.io/">[Project]</a>
            <a href="https://magivideogen.github.io/">[Code]</a>
            <br>
            <p>We introduce MAGI, a hybrid video generation framework that combines masked modeling for intra-frame generation with causal modeling for next-frame generation.</p>
          </td>
        </tr>

        <!-- Portrait4D-v2 -->
        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                  <source src='images/portrait4d-v2.mp4'>
                </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer</papertitle>
            <br>
            Yu Deng, <strong>Duomin Wang</strong>, Baoyuan Wang
            <br>
            <em>2024 European Conference on Computer Vision</em>, ECCV 2024,
            <br>
            <a href="https://arxiv.org/abs/2403.13570">[PDF]</a>
            <a href="https://yudeng.github.io/Portrait4D-v2/">[Project]</a>
            <a href="https://github.com/YuDeng/Portrait-4D">[Code]</a>
            <br>
            <p>We learn a lifelike 4D head synthesizer by creating pseudo multi-view videos from monocular ones as supervision.</p>
          </td>
        </tr>
        
        <!-- PICTURE -->
        <tr></tr>
          <!-- <td style="padding:20px;width:25%;vertical-align:middle" bgcolor="#ffffd0"> -->
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/picture.mp4 '>
              </video>
              <!-- <img src='images/picture.png' style="width:100%;max-width:100%; position: absolute;top: -5%"> -->
            </div>
          </td>
          <!-- <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0"> -->
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>PICTURE: PhotorealistIC Virtual Try-on from UnconstRained dEsigns</papertitle>
              <br>
              Shuliang Ning, <strong>Duomin Wang</strong>, Yipeng Qin, Zirong Jin, Baoyuan Wang, Xiaoguang Han
              <br>
              <em>2024 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2024,
              <br>
              <a href="https://arxiv.org/abs/2312.04534">[PDF]</a>
              <a href="https://ningshuliang.github.io/2023/Arxiv/index.html">[Project]</a>
              <a href="https://github.com/ningshuliang/PICTURE">[Code]</a>
              <a href="images/picture.txt">[BibTeX]</a>
              <br>
              <p>we propose a novel virtual try-on from unconstrained designs (ucVTON) task to enable photorealistic synthesis of personalized composite clothing on input human image.</p>
          </td>
        </tr>

        <!-- Portrait 4d -->
        <tr></tr>
          <!-- <td style="padding:20px;width:25%;vertical-align:middle" bgcolor="#ffffd0"> -->
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/portrait4d.mp4'>
              </video>
            </div>
          </td>
          <!-- <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0"> -->
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data</papertitle>
              <br>
              Yu Deng, <strong>Duomin Wang</strong>, Xiaohang Ren, Xingyu Chen, Baoyuan Wang
              <br>
              <em>2024 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2024,
              <br>
              <a href="https://arxiv.org/abs/2311.18729">[PDF]</a>
              <a href="https://yudeng.github.io/Portrait4D/">[Project]</a>
              <a href="https://github.com/YuDeng/Portrait-4D">[Code]</a>
              <a href="images/portrait4d.txt">[BibTeX]</a>
              <br>
              <p>We propose a one-shot 4D head synthesis approach for high-fidelity 4D head avatar reconstruction while trained on large-scale synthetic data.</p>
          </td>
        </tr>

        <!-- AgentAvatar -->
        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                      <source src='images/agentavatar.mp4'>
                  </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Disentangling Planning, Driving and Rendering for Photorealistic Avatar Agents</papertitle>
              <br>
              <strong>Duomin Wang</strong>, Bin Dai, Yu Deng, Baoyuan Wang
              <br>
              <em>2024 European Conference on Computer Vision, Workshop on EEC</em>, ECCVW 2024,
              <br>
              <a href="https://arxiv.org/abs/2311.17465.pdf">[PDF]</a>
              <a href="https://dorniwang.github.io/AgentAvatar_project/">[Project]</a>
              <a href="https://github.com/Dorniwang/AgentAvatar">[Code]</a>
              <a href="images/agentavatar.txt">[BibTeX]</a>
              <br>
              <p>We introduce a system that harnesses LLMs
                to produce a series of detailed text descriptions of the avatar
                agents' facial motions and then pro-
                cessed by our task-agnostic driving engine into motion to-
                ken sequences, which are subsequently converted into con-
                tinuous motion embeddings that are further consumed by
                our standalone neural-based renderer to generate the fi-
                nal photorealistic avatar animations.</p>
          </td>
      </tr>

      <!-- TH-PAD -->
      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                      <source src='images/thpad.mp4'>
                  </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors</papertitle>
              <br>
              Zhentao Yu, Zixin Yin, Deyu Zhou, <strong>Duomin Wang</strong>, Finn Wong, Baoyuan Wang
              <br>
              <em>2023 IEEE International Conference on Computer Vision</em>, ICCV 2023,
              <br>
              <a href="https://arxiv.org/abs/2212.04248">[PDF]</a>
              <a href="https://zxyin.github.io/TH-PAD">[Project]</a>
              <a href="">[Code(coming soon)]</a>
              <a href="images/thpad.txt">[BibTeX]</a>
              <br>
              <p>We introduce a simple and novel framework for one-shot audio-driven talking head generation. Unlike prior works that require additional driving sources for controlled synthesis in a deterministic manner, we instead probabilistically sample all the holistic lip-irrelevant facial motions (i.e. pose, expression, blink, gaze, etc.) to semantically match the input audio while still maintaining both the photo-realism of audio-lip synchronization and the overall naturalness.</p>
          </td>
      </tr>

      <!-- PD-FGC -->
      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                      <source src='images/pdfgc.mp4'>
                  </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis</papertitle>
              <br>
              <strong>Duomin Wang</strong>, Yu Deng, Zixin Yin, Heung-Yeung Shum, Baoyuan Wang
              <br>
              <em>2023 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2023,
              <br>
              <a href="https://arxiv.org/abs/2211.14506">[PDF]</a>
              <a href="https://dorniwang.github.io/PD-FGC/">[Project]</a>
              <a href="https://github.com/Dorniwang/PD-FGC-inference">[Code]</a>
              <a href="images/pdfgc.txt">[BibTeX]</a>
              <br>
              <p>We present a novel one-shot talking head synthesis method that achieves disentangled and fine-grained control over lip motion, eye gaze&blink, head pose, and emotional expression. We represent different motions via disentangled latent representations and leverage an image generator to synthesize talking heads from them.</p>
          </td>
        </tr>

      </td>
    </tr>
  </table>

  <!-- Academic Service -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Academic Service</heading>
        <p>
          <strong>Conference Reviewer</strong> WACV (2025), NeurIPS (2025), ACMMM (2025), ICCV (2025), ICLR (2025), CVPR (2025).
          <strong>Journal Reviewer</strong> IJCV.
        </p>
        
      </td>
    </tr>
      
</tbody></table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:2px;width:0%;vertical-align:middle">
      </td>
      <td style="padding:20px;width:100%;vertical-align:middle">
          <hr style="margin-top:0px">
          <p>The website template was adapted from <a href="https://yudeng.github.io/">Yu Deng</a>.</p>
      </td>
  </tr>
</tbody></table>

</body>

</html>
