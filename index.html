<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Duomin Wang">
  <meta name="description" content="Duomin Wang's Homepage">
  <meta name="keywords" content="Duomin Wang,王多民,homepage,主页,computer vision,xiaobing.ai,Jilin University,talking head synthesis,representation learning,facial landmark detection,3D face reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="n8x1FW7r1Wku3m83FO96GU0x1exw46HxbsNW1IUM12U" />
  <title>Duomin Wang (王多民)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Duomin Wang (王多民)</name>
              </p>
              <p style="text-align:center">
                Email: wangduomin[at]xiaobing.ai &nbsp; &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=a6GrP_EAAAAJ&hl=en">Google Scholar</a> &nbsp; &nbsp;&nbsp;&nbsp;<a href="https://github.com/Dorniwang">Github</a>
              </p>
              <p>I am currently a researcher at <a href="https://www.linkedin.com/company/xiaobing-ai">Xiaobing.ai</a> from 2021. My research interests include talking head synthesis, representation learning, disentanglement and 3D face reconstruction. 
              </p>
              <p>
                Before joining Xiaobing, I was worked at OPPO Research Institute for three years, my research results are applied to the camera software of OPPO mobile phones as the basic face algorithm.
              </p>
              <p>
                <strong>
                I'm seeking research interns on talking head synthesis and representation learning. Feel free to send me an email if you are interested.
                </strong>
              </p>
		    
            </td>
            <td style="padding:15% 7% 7% 7%;width:40%;max-width:40%">
              <a href="images/duomin.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/duomin.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <!-- News -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <strong>2023/07/14</strong> &nbsp Had one paper accepted by ICCV 2023 about talking head sythesis (<a href="https://zxyin.github.io/TH-PAD/">TH-PAD</a>).
            </p>
            <p>
              <strong>2023/07/10</strong> &nbsp Our CVPR 2023 work <a href="https://dorniwang.github.io/PD-FGC/">PD-FGC</a> has released the code and model, check it out!.
            </p>
            <p>
              <strong>2023/02/28</strong> &nbsp Had one paper accepted by CVPR 2023 about talking head sythesis (<a href="https://dorniwang.github.io/PD-FGC/">PD-FGC</a>).
            </p>
          </td>
        </tr>
      </tbody></table>

      <!-- publications -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                      <source src='images/thpad.mp4'>
                  </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors</papertitle>
              <br>
              Zhentao Yu, Zixin Yin, Deyu Zhou, <strong>Duomin Wang</strong>, Finn Wong, Baoyuan Wang
              <br>
              <em>2023 IEEE International Conference on Computer Vision</em>, ICCV 2023,
              <br>
              <a href="https://arxiv.org/abs/2212.04248">[PDF]</a>
              <a href="https://zxyin.github.io/TH-PAD">[Project]</a>
              <a href="">[Code(coming soon)]</a>
              <a href="images/thpad.txt">[BibTeX]</a>
              <br>
              <p>We introduce a simple and novel framework for one-shot audio-driven talking head generation. Unlike prior works that require additional driving sources for controlled synthesis in a deterministic manner, we instead probabilistically sample all the holistic lip-irrelevant facial motions (i.e. pose, expression, blink, gaze, etc.) to semantically match the input audio while still maintaining both the photo-realism of audio-lip synchronization and the overall naturalness.</p>
          </td>
      </tr>

        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                      <source src='images/pdfgc.mp4'>
                  </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis</papertitle>
              <br>
              <strong>Duomin Wang</strong>, Yu Deng, Zixin Yin, Heung-Yeung Shum, Baoyuan Wang
              <br>
              <em>2023 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2023,
              <br>
              <a href="https://arxiv.org/abs/2211.14506">[PDF]</a>
              <a href="https://dorniwang.github.io/PD-FGC/">[Project]</a>
              <a href="https://github.com/Dorniwang/PD-FGC-inference">[Code]</a>
              <a href="images/pdfgc.txt">[BibTeX]</a>
              <br>
              <p>We present a novel one-shot talking head synthesis method that achieves disentangled and fine-grained control over lip motion, eye gaze&blink, head pose, and emotional expression. 
                   We represent different motions via disentangled latent representations and leverage an image generator to synthesize talking heads from them.</p>
          </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:0%;vertical-align:middle">
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
		<hr style="margin-top:0px">
                <p>The website template was adapted from <a href="https://yudeng.github.io/">Yu Deng</a>.</p>
            </td>
        </tr>

      </td>
    </tr>
  </table>
</body>

</html>
